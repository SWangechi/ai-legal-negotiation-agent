{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b4b74e8",
   "metadata": {},
   "source": [
    "# Data Cleaning + Embedding + FAISS RAG (OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0600be2",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "Build a complete RAG (Retrieval-Augmented Generation) pipeline using:\n",
    "\n",
    "- Legal/negotiation/contract documents\n",
    "\n",
    "- Document cleaning + normalization\n",
    "\n",
    "- Text chunking\n",
    "\n",
    "- OpenAI embeddings (text-embedding-3-small)\n",
    "\n",
    "- FAISS vectorstore (Windows-compatible)\n",
    "\n",
    "- Semantic search + RAG answer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb76052",
   "metadata": {},
   "source": [
    "## 1. Imports & configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f12e3ed",
   "metadata": {},
   "source": [
    "Install all dependencies required for:\n",
    "\n",
    "- Environment variable loading\n",
    "\n",
    "- PDF/text reading\n",
    "\n",
    "- Embedding generation\n",
    "\n",
    "- FAISS vector storage\n",
    "\n",
    "- Tracking progress\n",
    "\n",
    "Key libraries:\n",
    "\n",
    "- python-dotenv for API key loading\n",
    "\n",
    "- openai (SDK v1)\n",
    "\n",
    "- pandas for tracking cleaned files\n",
    "\n",
    "- tqdm for progress bars\n",
    "\n",
    "- PyPDF2 for reading PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac52c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Documents\\ai-legal-negotiation-agent\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.auto import tqdm\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "from openai import OpenAI\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "except Exception:\n",
    "    OpenAI = None\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception:\n",
    "    SentenceTransformer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1813817f",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9879d",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "- Import Python libraries used throughout the notebook\n",
    "\n",
    "- Load .env\n",
    "\n",
    "- Initialize OpenAI client\n",
    "\n",
    "**What happens:**\n",
    "\n",
    "- API key validation\n",
    "\n",
    "- Prevents runtime errors early\n",
    "\n",
    "- Creates a reusable OpenAI client instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f7ac01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in .env file!\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "print(\"OpenAI API Key loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e40df6",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "Automatically detect your project root without hardcoding paths.\n",
    "Helps avoid file path errors on different machines.\n",
    "\n",
    "Folders checked:\n",
    "\n",
    "- /data\n",
    "\n",
    "- /backend\n",
    "\n",
    "- /frontend\n",
    "\n",
    "**Also sets:**\n",
    "\n",
    "- Raw documents directory\n",
    "\n",
    "- Cleaned documents directory\n",
    "\n",
    "- Vectorstore directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb3e052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = C:\\Users\\user\\Documents\\ai-legal-negotiation-agent\\ai-legal-negotiation-agent\n",
      "RAW_DIR: C:\\Users\\user\\Documents\\ai-legal-negotiation-agent\\ai-legal-negotiation-agent\\data\\raw\\generated\n",
      "CLEAN_DIR: C:\\Users\\user\\Documents\\ai-legal-negotiation-agent\\ai-legal-negotiation-agent\\data\\clean_openai\n",
      "CHROMA_DIR: C:\\Users\\user\\Documents\\ai-legal-negotiation-agent\\ai-legal-negotiation-agent\\data\\vectorstore_openai\n"
     ]
    }
   ],
   "source": [
    "def find_project_root(start=Path.cwd(), markers=(\"data\", \"backend\", \"frontend\")):\n",
    "    cur = start.resolve()\n",
    "    for _ in range(10):\n",
    "        if any((cur / m).exists() for m in markers):\n",
    "            return cur\n",
    "        if cur == cur.parent:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    raise FileNotFoundError(\"Project root not found.\")\n",
    "\n",
    "try:\n",
    "    PROJECT_ROOT = find_project_root()\n",
    "except:\n",
    "    PROJECT_ROOT = Path(r\"C:\\Users\\user\\Documents\\ai-legal-negotiation-agent\\ai-legal-negotiation-agent\")\n",
    "\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n",
    "\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"generated\"\n",
    "CLEAN_DIR = PROJECT_ROOT / \"data\" / \"clean_openai\"\n",
    "CHROMA_DIR = PROJECT_ROOT / \"data\" / \"vectorstore_openai\"\n",
    "\n",
    "print(\"RAW_DIR:\", RAW_DIR)\n",
    "print(\"CLEAN_DIR:\", CLEAN_DIR)\n",
    "print(\"CHROMA_DIR:\", CHROMA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339beda",
   "metadata": {},
   "source": [
    "## 3. List Raw files and confirm count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cb1174",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "Display all raw documents in your project.\n",
    "\n",
    "Useful for verifying file availability before cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a0dca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Found 56 raw documents.\n",
      " - contract_10_20251112_173721.txt\n",
      " - contract_11_20251112_173748.txt\n",
      " - contract_12_20251112_173817.txt\n",
      " - contract_1_20251112_173315.txt\n",
      " - contract_2_20251112_173345.txt\n",
      " - contract_3_20251112_173421.txt\n",
      " - contract_4_20251112_173454.txt\n",
      " - contract_5_20251112_173519.txt\n",
      " - contract_6_20251112_173542.txt\n",
      " - contract_7_20251112_173610.txt\n"
     ]
    }
   ],
   "source": [
    "raw_files = sorted(RAW_DIR.glob(\"*\"))\n",
    "\n",
    "print(f\"ðŸ“„ Found {len(raw_files)} raw documents.\")\n",
    "for f in raw_files[:10]:\n",
    "    print(\" -\", f.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c003c61",
   "metadata": {},
   "source": [
    "## 4. Cleaning Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3844c",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "Defines helper functions for:\n",
    "\n",
    "- Reading text files\n",
    "\n",
    "- Extracting PDF text\n",
    "\n",
    "- Cleaning messy text\n",
    "\n",
    "- Removing double spaces, extra newlines\n",
    "\n",
    "- Fingerprinting documents (SHA-256) to identify duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "536108dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning helpers ready.\n"
     ]
    }
   ],
   "source": [
    "def read_txt(path):\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def read_pdf(path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        pdf = PdfReader(str(path))\n",
    "        for page in pdf.pages:\n",
    "            text += (page.extract_text() or \"\") + \"\\n\"\n",
    "    except:\n",
    "        pass\n",
    "    return text\n",
    "\n",
    "def sanitize(text):\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\")\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def fingerprint(text):\n",
    "    return hashlib.sha256(text.encode()).hexdigest()\n",
    "\n",
    "print(\"Cleaning helpers ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c63ffb1",
   "metadata": {},
   "source": [
    "## 5. Clean, deduplicate, and write cleaned files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc8d18",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "- Read every raw document\n",
    "\n",
    "- Convert to clean normalized text\n",
    "\n",
    "- Skip documents that are too short\n",
    "\n",
    "- Skip duplicates (via fingerprint hashing)\n",
    "\n",
    "- Save cleaned version to /data/clean_openai\n",
    "\n",
    "- Log metadata into a pandas DataFrame\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "- Count of cleaned files\n",
    "\n",
    "- Count of skipped short files\n",
    "\n",
    "- Count of skipped duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e21ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56/56 [00:00<00:00, 369.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned documents: 56\n",
      "Skipped short: 0\n",
      "Skipped duplicates: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "clean_file",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "chars",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "19cd224e-07be-4642-a50a-b01e28475078",
       "rows": [
        [
         "0",
         "contract_10_20251112_173721.txt",
         "contract_10_20251112_173721_clean.txt",
         "4907"
        ],
        [
         "1",
         "contract_11_20251112_173748.txt",
         "contract_11_20251112_173748_clean.txt",
         "5369"
        ],
        [
         "2",
         "contract_12_20251112_173817.txt",
         "contract_12_20251112_173817_clean.txt",
         "6359"
        ],
        [
         "3",
         "contract_1_20251112_173315.txt",
         "contract_1_20251112_173315_clean.txt",
         "4469"
        ],
        [
         "4",
         "contract_2_20251112_173345.txt",
         "contract_2_20251112_173345_clean.txt",
         "4850"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>clean_file</th>\n",
       "      <th>chars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>contract_10_20251112_173721.txt</td>\n",
       "      <td>contract_10_20251112_173721_clean.txt</td>\n",
       "      <td>4907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contract_11_20251112_173748.txt</td>\n",
       "      <td>contract_11_20251112_173748_clean.txt</td>\n",
       "      <td>5369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>contract_12_20251112_173817.txt</td>\n",
       "      <td>contract_12_20251112_173817_clean.txt</td>\n",
       "      <td>6359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>contract_1_20251112_173315.txt</td>\n",
       "      <td>contract_1_20251112_173315_clean.txt</td>\n",
       "      <td>4469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>contract_2_20251112_173345.txt</td>\n",
       "      <td>contract_2_20251112_173345_clean.txt</td>\n",
       "      <td>4850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename                             clean_file  \\\n",
       "0  contract_10_20251112_173721.txt  contract_10_20251112_173721_clean.txt   \n",
       "1  contract_11_20251112_173748.txt  contract_11_20251112_173748_clean.txt   \n",
       "2  contract_12_20251112_173817.txt  contract_12_20251112_173817_clean.txt   \n",
       "3   contract_1_20251112_173315.txt   contract_1_20251112_173315_clean.txt   \n",
       "4   contract_2_20251112_173345.txt   contract_2_20251112_173345_clean.txt   \n",
       "\n",
       "   chars  \n",
       "0   4907  \n",
       "1   5369  \n",
       "2   6359  \n",
       "3   4469  \n",
       "4   4850  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "records = []\n",
    "seen = set()\n",
    "skipped_short = skipped_dupe = 0\n",
    "\n",
    "for f in tqdm(raw_files, desc=\"Cleaning\"):\n",
    "    raw = read_pdf(f) if f.suffix.lower() == \".pdf\" else read_txt(f)\n",
    "    clean = sanitize(raw)\n",
    "\n",
    "    if len(clean) < 200:\n",
    "        skipped_short += 1\n",
    "        continue\n",
    "\n",
    "    fp = fingerprint(clean)\n",
    "    if fp in seen:\n",
    "        skipped_dupe += 1\n",
    "        continue\n",
    "    seen.add(fp)\n",
    "\n",
    "    out = CLEAN_DIR / (f.stem + \"_clean.txt\")\n",
    "    out.write_text(clean, encoding=\"utf-8\")\n",
    "\n",
    "    records.append({\"filename\": f.name, \"clean_file\": out.name, \"chars\": len(clean)})\n",
    "\n",
    "print(\"Cleaned documents:\", len(records))\n",
    "print(\"Skipped short:\", skipped_short)\n",
    "print(\"Skipped duplicates:\", skipped_dupe)\n",
    "\n",
    "df_clean = pd.DataFrame(records)\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c779e223",
   "metadata": {},
   "source": [
    "## 6. Chunking Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57eb130f",
   "metadata": {},
   "source": [
    "**Purpose:**\n",
    "\n",
    "Split cleaned documents into overlapping chunks (for embeddings).\n",
    "Chunking improves search recall and avoids embedding very long texts.\n",
    "\n",
    "Defaults:\n",
    "\n",
    "- Max length = 1500 characters\n",
    "\n",
    "- Overlap = 200 characters\n",
    "\n",
    "**Result:**\n",
    "A list of chunk dictionaries containing:\n",
    "\n",
    "- id\n",
    "\n",
    "- doc (chunk text)\n",
    "\n",
    "- source (original file name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b98817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': '1_chunk_1',\n",
       "  'doc': '**LEASE AGREEMENT**\\n\\n**THIS LEASE AGREEMENT** (hereinafter referred to as the \"Agreement\") is made and entered into this ___ day of __________, 20__, by and between:\\n\\n**LANDLORD:** \\nName: ______________________________ \\nID Number: _________________________ \\nAddress: ____________________________ \\nPhone Number: ______________________ \\n\\n**AND**\\n\\n**TENANT:** \\nName: ______________________________ \\nID Number: _________________________ \\nAddress: ____________________________ \\nPhone Number: ______________________ \\n\\n**WHEREAS**, the Landlord is the lawful owner of the property located at: \\n**Property Address:** ____________________________________________________ \\n(hereinafter referred to as the \"Premises\"); \\n\\n**AND WHEREAS**, the Tenant wishes to lease the Premises from the Landlord under the terms and conditions set forth herein.\\n\\n**NOW, THEREFORE, in consideration of the mutual covenants and agreements contained herein, the parties agree as follows:**\\n\\n### 1. **LEASE TERM**\\nThe lease shall commence on the ___ day of __________, 20__ (the \"Commencement Date\") and shall continue for a period of twelve (12) months, ending on the ___ day of __________, 20__ (the \"Termination Date\"), unless earlier terminated in accordance with the provisions of this Agreement.\\n\\n### 2. **RENT**\\n2.1 The Tenant agrees to pay the Landlord a monthly rent of Ksh. __________ (the \"Rent\"). \\n2.2 Rent shall be payable in advance on or before the first day of each month. \\n2.3 Payment shall be made via bank transfe',\n",
       "  'source': 'contract_10_20251112_173721.txt'},\n",
       " {'id': '1_chunk_2',\n",
       "  'doc': 't agrees to pay the Landlord a monthly rent of Ksh. __________ (the \"Rent\"). \\n2.2 Rent shall be payable in advance on or before the first day of each month. \\n2.3 Payment shall be made via bank transfer to the following account: \\n Bank Name: __________________________ \\n Account Number: ______________________ \\n Account Name: ________________________ \\n\\n### 3. **SECURITY DEPOSIT**\\n3.1 The Tenant shall pay a security deposit of Ksh. __________ (the \"Security Deposit\") upon signing this Agreement. \\n3.2 The Security Deposit shall be held by the Landlord as security for the performance of the Tenant\\'s obligations under this Agreement. \\n3.3 The Landlord shall return the Security Deposit to the Tenant within 30 days after the Termination Date, less any deductions for damages beyond normal wear and tear.\\n\\n### 4. **USE OF PREMISES**\\n4.1 The Premises shall be used exclusively for residential purposes. \\n4.2 The Tenant shall not make any alterations or improvements to the Premises without the prior written consent of the Landlord.\\n\\n### 5. **MAINTENANCE AND REPAIRS**\\n5.1 The Landlord shall be responsible for maintaining the structural integrity of the Premises, including plumbing, electrical systems, and major appliances. \\n5.2 The Tenant shall maintain the Premises in a clean and sanitary condition and shall be responsible for any damage caused by the Tenant\\'s negligence.\\n\\n### 6. **UTILITIES**\\nThe Tenant shall be responsible for all utility charges associated with the Premises, including but',\n",
       "  'source': 'contract_10_20251112_173721.txt'},\n",
       " {'id': '1_chunk_3',\n",
       "  'doc': \"ion and shall be responsible for any damage caused by the Tenant's negligence.\\n\\n### 6. **UTILITIES**\\nThe Tenant shall be responsible for all utility charges associated with the Premises, including but not limited to water, electricity, and internet services.\\n\\n### 7. **TERMINATION**\\n7.1 Either party may terminate this Agreement by providing the other party with at least thirty (30) days written notice prior to the intended termination date. \\n7.2 The Landlord may terminate this Agreement immediately for any breach by the Tenant of the terms herein.\\n\\n### 8. **DEFAULT**\\nIn the event of default by the Tenant in the payment of Rent or any other obligations under this Agreement, the Landlord shall have the right to terminate this Agreement and take possession of the Premises after giving the Tenant a notice of default.\\n\\n### 9. **GOVERNING LAW**\\nThis Agreement shall be governed by and construed in accordance with the laws of the Republic of Kenya.\\n\\n### 10. **DISPUTE RESOLUTION**\\n10.1 Any disputes arising out of or in connection with this Agreement shall first be referred to mediation. \\n10.2 If mediation fails, the parties agree to refer the matter to arbitration in accordance with the Arbitration Act of Kenya.\\n\\n### 11. **ENTIRE AGREEMENT**\\nThis Agreement constitutes the entire agreement between the parties and supersedes all prior negotiations, representations, and agreements, whether written or oral.\\n\\n### 12. **AMENDMENTS**\\nNo amendment to this Agreement shall be effective unless in\",\n",
       "  'source': 'contract_10_20251112_173721.txt'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_text(text, max_len=1500, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + max_len\n",
    "        chunks.append(text[start:end].strip())\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for i, row in df_clean.iterrows():\n",
    "    text = (CLEAN_DIR / row[\"clean_file\"]).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    \n",
    "    parts = chunk_text(text)\n",
    "    for j, c in enumerate(parts):\n",
    "        chunks.append({\n",
    "            \"id\": f\"{i+1}_chunk_{j+1}\",\n",
    "            \"doc\": c,\n",
    "            \"source\": row[\"filename\"]\n",
    "        })\n",
    "\n",
    "print(\"Total chunks created:\", len(chunks))\n",
    "chunks[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49af4c",
   "metadata": {},
   "source": [
    "## 7. Generate OpenAI Embeddings\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Use OpenAIâ€™s text-embedding-3-small model to convert each text chunk into a numerical vector.\n",
    "\n",
    "**Features:**\n",
    "\n",
    "- Batch processing (32 per request)\n",
    "\n",
    "- Embeddings, IDs, metadata stored in lists\n",
    "\n",
    "**Result:**\n",
    "\n",
    "- embeddings â†’ List of embedding vectors\n",
    "\n",
    "- ids â†’ Chunk IDs\n",
    "\n",
    "- metadata â†’ Original filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3304b59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:12<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings complete: 227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def embed(text_batch):\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text_batch\n",
    "    )\n",
    "    return [d.embedding for d in response.data]\n",
    "\n",
    "embeddings = []\n",
    "ids = []\n",
    "metadata = []\n",
    "\n",
    "BATCH = 32\n",
    "print(\"Generating embeddings...\")\n",
    "\n",
    "for i in tqdm(range(0, len(chunks), BATCH), desc=\"Embedding\"):\n",
    "    batch = chunks[i:i+BATCH]\n",
    "    texts = [x[\"doc\"] for x in batch]\n",
    "\n",
    "    embs = embed(texts)\n",
    "\n",
    "    for c, e in zip(batch, embs):\n",
    "        embeddings.append(e)\n",
    "        ids.append(c[\"id\"])\n",
    "        metadata.append({\"source\": c[\"source\"]})\n",
    "\n",
    "print(\"Embeddings complete:\", len(embeddings))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc5ba0",
   "metadata": {},
   "source": [
    "## 8. Build FAISS Vectorstore (Windows Compatible)\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Create and store a FAISS vector index for fast similarity search.\n",
    "\n",
    "Why FAISS instead of Chroma:\n",
    "\n",
    "- Chroma 0.5+ breaks old API\n",
    "\n",
    "- Chroma persistence crashes on Windows\n",
    "\n",
    "- FAISS works flawlessly and is production-grade\n",
    "\n",
    "**What this cell does:**\n",
    "\n",
    "- Creates a FAISS L2 index\n",
    "\n",
    "- Converts embeddings to float32\n",
    "\n",
    "- Adds embeddings to FAISS\n",
    "\n",
    "- Saves index + metadata to disk\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "- documents.index â†’ FAISS binary index\n",
    "\n",
    "- metadata.pkl â†’ Chunk metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6aaad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index size: 227\n",
      "FAISS vectorstore saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "VECTOR_DIM = len(embeddings[0])\n",
    "\n",
    "index = faiss.IndexFlatL2(VECTOR_DIM)\n",
    "\n",
    "emb_matrix = np.array(embeddings).astype('float32')\n",
    "\n",
    "index.add(emb_matrix)\n",
    "\n",
    "print(\"FAISS index size:\", index.ntotal)\n",
    "\n",
    "FAISS_DIR = PROJECT_ROOT / \"data\" / \"vectorstore_faiss\"\n",
    "FAISS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "faiss.write_index(index, str(FAISS_DIR / \"documents.index\"))\n",
    "\n",
    "with open(FAISS_DIR / \"metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"ids\": ids,\n",
    "        \"chunks\": chunks,\n",
    "        \"metadata\": metadata\n",
    "    }, f)\n",
    "\n",
    "print(\"FAISS vectorstore saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d80e2",
   "metadata": {},
   "source": [
    "## 9. Load FAISS + Metadata\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Load FAISS and metadata in any new session.\n",
    "Allows retrieval without recomputing embeddings.\n",
    "\n",
    "Loads:\n",
    "\n",
    "- FAISS index\n",
    "\n",
    "- Chunk text\n",
    "\n",
    "- Metadata\n",
    "\n",
    "- Document IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8feeeb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded FAISS index with size: 227\n",
      "Loaded chunks: 227\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "FAISS_DIR = PROJECT_ROOT / \"data\" / \"vectorstore_faiss\"\n",
    "\n",
    "index = faiss.read_index(str(FAISS_DIR / \"documents.index\"))\n",
    "print(\"Loaded FAISS index with size:\", index.ntotal)\n",
    "\n",
    "with open(FAISS_DIR / \"metadata.pkl\", \"rb\") as f:\n",
    "    store = pickle.load(f)\n",
    "\n",
    "ids = store[\"ids\"]\n",
    "chunks = store[\"chunks\"]\n",
    "metadata = store[\"metadata\"]\n",
    "\n",
    "print(\"Loaded chunks:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594f955",
   "metadata": {},
   "source": [
    "## 10. Semantic Search Using OpenAI Embeddings\n",
    "\n",
    "**Purpose:**\n",
    "Implement hybrid search:\n",
    "\n",
    "- Embed the user query\n",
    "\n",
    "- Search the FAISS index\n",
    "\n",
    "- Return the most relevant chunks\n",
    "\n",
    "**Outputs for each match:**\n",
    "\n",
    "- Chunk text\n",
    "\n",
    "- Original document name\n",
    "\n",
    "- L2 distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fb243f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk': 'ovided and the terms of payment in accordance with the agreed conditions.)',\n",
       "  'source': 'contracts_7.txt',\n",
       "  'distance': 0.8464417457580566},\n",
       " {'chunk': \"constraints and seeks to negotiate a more favorable rate. The Client presents the following considerations:\\n- Budget Cap: KES [Client's Budget Cap]\\n- Duration of the Project: [Duration]\\n- Expected Outcomes: [Describe expected outcomes]\\n\\n**5. Negotiation Process**\\n\\nThe parties agree to the following steps for negotiation:\\n- Initial Meeting: A meeting will be held on [Insert Date] at [Insert Time] to discuss the proposed rates.\\n- Counter-Proposal: The Client will provide a counter-proposal no later than [Insert Date].\\n- Final Agreement: Both parties aim to reach a final agreement by [Insert Date].\\n\\n**6. Factors Influencing Rate Adjustment**\\n\\nBoth parties acknowledge that the following factors may influence the final agreed rate:\\n- Market Trends: Analysis of current market rates for similar services.\\n- Experience Level: The Freelancerâ€™s years of experience and portfolio of past work.\\n- Project Specifics: The complexity of the tasks outlined in the scope of services.\\n\\n**7. Payment Terms**\\n\\nUpon reaching an agreement, the following payment terms will apply:\\n- Payment Schedule: [Insert payment milestones, e.g., 50% upfront, 50% upon completion]\\n- Payment Method: [Bank transfer, M-Pesa, etc.]\\n- Invoicing: The Freelancer will issue invoices upon completion of each milestone.\\n\\n**8. Confidentiality**\\n\\nBoth parties agree to maintain confidentiality regarding sensitive information shared during the negotiation process and throughout the duration of the project.\\n\\n**9. Dispute Resolution**\",\n",
       "  'source': 'negotiation_5_20251112_174509.txt',\n",
       "  'distance': 0.9803347587585449},\n",
       " {'chunk': '# Contract Terms Negotiation between Supplier and Retailer\\n\\n## Context\\nThis negotiation involves a supplier (hereinafter referred to as â€œSupplierâ€) providing consumer goods to a retailer (hereinafter referred to as â€œRetailerâ€) in Kenya. Both parties aim to establish a mutually beneficial agreement that outlines the terms of supply, payment, delivery, and other critical aspects of their business relationship.\\n\\n## Parties Involved\\n- **Supplier:** ABC Distributors Ltd., a company registered under the Companies Act of Kenya.\\n- **Retailer:** XYZ Retailers Ltd., a company registered under the Companies Act of Kenya.\\n\\n## Date of Negotiation\\n- [Date]\\n\\n## Location of Negotiation\\n- [Location]\\n\\n---\\n\\n### Section 1: Scope of Supply\\n**1.1 Product Description** \\nThe Supplier agrees to supply the following products to the Retailer:\\n- Consumer electronics (e.g., mobile phones, laptops)\\n- Household appliances (e.g., refrigerators, microwaves)\\n\\n**1.2 Quantity** \\nThe Supplier will provide a minimum order quantity of [insert quantity] units per month, with adjustments based on demand and mutual agreement.\\n\\n### Section 2: Pricing\\n**2.1 Unit Price** \\nThe unit price for each product will be determined as follows:\\n- [Insert unit price] for each product listed in Section 1.1.\\n\\n**2.2 Price Adjustments** \\nAny price adjustments will be communicated in writing at least [30] days in advance, based on market conditions, production costs, or other relevant factors.\\n\\n### Section 3: Payment Terms\\n**3.1 Payment',\n",
       "  'source': 'negotiation_3_20251112_174423.txt',\n",
       "  'distance': 0.9897557497024536}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def embed_query(text):\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=[text]\n",
    "    )\n",
    "    return np.array(response.data[0].embedding).astype(\"float32\")\n",
    "\n",
    "\n",
    "def search_faiss(query, k=5):\n",
    "    query_vec = embed_query(query).reshape(1, -1)\n",
    "    distances, indices = index.search(query_vec, k)\n",
    "    \n",
    "    results = []\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        results.append({\n",
    "            \"chunk\": chunks[idx][\"doc\"],\n",
    "            \"source\": chunks[idx][\"source\"],\n",
    "            \"distance\": float(dist)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "search_faiss(\"payment terms in the negotiation contract\", k=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6ee7c0",
   "metadata": {},
   "source": [
    "## 11. RAG Answer Generation using OpenAI\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Build a RAG (Retrieval-Augmented Generation) prompt using the retrieved chunks.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "- Retrieve top-k similar chunks\n",
    "\n",
    "- Build a context block\n",
    "\n",
    "- Ask OpenAI to answer ONLY using those chunks\n",
    "\n",
    "- Return answer + documents used\n",
    "\n",
    "**Uses:**\n",
    "\n",
    "- gpt-4.1-mini (fast, low cost, accurate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c992e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(query, k=5):\n",
    "    results = search_faiss(query, k=k)\n",
    "    \n",
    "    context = \"\\n\\n---\\n\\n\".join([r[\"chunk\"] for r in results])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an AI legal assistant. Use ONLY the context below to answer the user's question.\n",
    "If the answer is not in the documents, say: \"The documents do not contain that information.\"\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=350,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer, results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2189fb7b",
   "metadata": {},
   "source": [
    "## 12. Test the RAG Pipeline\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Verify that the system retrieves correct documents and generates a grounded, context-based answer.\n",
    "\n",
    "**Output:**\n",
    "\n",
    "- Final legal-style answer\n",
    "\n",
    "- List of chunks used\n",
    "\n",
    "- Relevance distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acbcce79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RAG Answer:\n",
      "\n",
      "The payment terms in the negotiation contract are as follows:\n",
      "\n",
      "- Payment Schedule: To be determined and inserted (e.g., 50% upfront, 50% upon completion).\n",
      "- Payment Method: To be specified (e.g., Bank transfer, M-Pesa, etc.).\n",
      "- Invoicing: The Freelancer will issue invoices upon completion of each milestone.\n",
      "\n",
      "Additionally, for the supplier-retailer agreement:\n",
      "\n",
      "- Payment terms are to be detailed in Section 3.1 (not fully provided in the context).\n",
      "- Any price adjustments must be communicated in writing at least 30 days in advance.\n",
      "\n",
      "The exact payment schedule and methods are to be agreed upon and inserted in the contract.\n",
      "\n",
      "\n",
      " Retrieved Documents Used:\n",
      "\n",
      "- Source: negotiation_5_20251112_174509.txt (distance=0.8834)\n",
      "- Source: contracts_7.txt (distance=0.8947)\n",
      "- Source: negotiation_3_20251112_174423.txt (distance=0.9425)\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the payment terms in the negotiation contract?\"\n",
    "answer, docs = rag_answer(query, k=3)\n",
    "\n",
    "print(\" RAG Answer:\\n\")\n",
    "print(answer)\n",
    "\n",
    "print(\"\\n\\n Retrieved Documents Used:\\n\")\n",
    "for d in docs:\n",
    "    print(f\"- Source: {d['source']} (distance={d['distance']:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
